import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import zscore

# Load the dataset
data = pd.read_csv(r'D:\internship\data analytics\task4\house_prices.csv')


# 1. Inspect the Dataset
print("Dataset Overview:")
print(data.info())  # Summary of the dataset
print("\nFirst 5 Rows of the Dataset:")
print(data.head())  # Display the first 5 rows
# 2. Check for Missing Values
print("\nMissing Values:")
print(data.isnull().sum())  # Count missing values in each column

# Handle missing values
# For numerical columns, fill missing values with the median
numerical_columns = ['Size (sq ft)', 'Number of Rooms', 'Price ($)']
for col in numerical_columns:
    if col in data.columns:  # Check if the column exists
        data[col] = data[col].astype(str).str.replace(",", "").str.strip()
        data[col] = pd.to_numeric(data[col], errors='coerce')  # Convert to numeric, coerce invalid to NaN
        if data[col].isnull().sum() > 0:
            data[col] = data[col].fillna(data[col].median())
    else:
        print(f"Warning: Column '{col}' not found in dataset.")

# Handle missing values for categorical columns
categorical_columns = ['Location']
for col in categorical_columns:
    if col in data.columns:  # Ensure the column exists
        if data[col].isnull().sum() > 0:
            data[col] = data[col].fillna(data[col].mode()[0])
    else:
        print(f"Warning: Column '{col}' not found in dataset.")
# Verify missing values after handling
print("\nMissing Values After Handling:")
print(data.isnull().sum())
# 3. Analyze Distributions of Numerical Variables
plt.figure(figsize=(15, 5))

for i, col in enumerate(numerical_columns):
    plt.subplot(1, 3, i + 1)
    sns.histplot(data[col], kde=True, bins=30)
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")

plt.tight_layout()
plt.show()

from sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder
import pandas as pd
# Step 1: Normalize Numerical Features
numerical_columns = ['Size (sq ft)', 'Number of Rooms']
scaler = MinMaxScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])
# Step 2: Encode Categorical Features
#One-Hot Encoding for non-ordinal categories
data = pd.get_dummies(data, columns=['Location'], drop_first=True)
# Display the preprocessed dataset
print("Preprocessed Data:")
print(data)

# Step 1: Compute Correlation Matrix
correlation_matrix = data.corr()

# Step 2: Visualize Correlation Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Correlation Matrix")
plt.show()

# Step 3: Analyze Correlation with the Target Variable (Price)
correlation_with_price = correlation_matrix['Price ($)'].sort_values(ascending=False)
print("Correlation with Price:")
print(correlation_with_price)

# Step 4: Identify Low-Impact Predictors (Threshold = 0.1)
low_impact_predictors = correlation_with_price[correlation_with_price.abs() < 0.1].index.tolist()
if len(low_impact_predictors) > 0:
    print("\nLow-Impact Predictors (to consider removing):", low_impact_predictors)
else:
    print("\nNo low-impact predictors")

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Step 1: Define Features and Target
X = data.drop(columns=['Price ($)'])  # Features (independent variables)
y = data['Price ($)']  # Target (dependent variable)

# Step 2: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Train-Test Split Completed:")
print(f"Training Data: {X_train.shape[0]} samples")
print(f"Testing Data: {X_test.shape[0]} samples\n")

# Step 3: Train Linear Regression Model
model = LinearRegression()  # Initialize the model
model.fit(X_train, y_train)  # Train the model

print("Model Training Completed.\n")

# Step 4: Evaluate the Model
y_pred = model.predict(X_test)  # Make predictions on test set

# Calculate Evaluation Metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("Model Evaluation Metrics:")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R² Score: {r2:.2f}\n")

# Step 5: Display Model Coefficients
print("Model Coefficients:")
for feature, coef in zip(X.columns, model.coef_):
    print(f"{feature}: {coef:.2f}")

print(f"Intercept: {model.intercept_:.2f}")    

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Step 1: Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# Step 2: Calculate R² Score
r2 = r2_score(y_test, y_pred)

# Display the results
print("Model Evaluation Results:")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R² (Coefficient of Determination): {r2:.2f}")
